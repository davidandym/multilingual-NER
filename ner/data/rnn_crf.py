"""
Generator and Predictor classes for the word-level BiRNN CRF Model for NER.
"""


# pylint: disable=too-many-locals
# pylint: disable=too-many-instance-attributes
# pylint: disable=too-many-statements
# pylint: disable=invalid-name


import tensorflow as tf

import ner.data.dataset as data
from ner.features import Features, Predictions
from ner.registry import Registries
from ner.data.io import get_vocabs


@Registries.train_generators.register("standard_word_crf")
@Registries.test_generators.register("standard_word_crf")
class ClassicModelGenerator(data.Generator):
    """ Classic Model Data Generator.

        Each example is one sentence in the dataset.
        Features include indexed words, and character or byte representations
        of each word.

        Requires a decent number of hyperparameters, I'll enumerate them here:
            - vocabs: Your vocab object, usually generated by
              ../scripts/build_vocab.py. If using words as features,
              should have a word indexer and a singleton list. Must include a
              output tag indexer.

            - features: a string containing the features you want. typically
              'words+bytes' or something to that effect.

            - char_pad_len: How much to pad the char/byte representation by.

            - drop_single_rate: The rate at which to drop singletons words and
              replace them with the UNK token. This has been shown to help with
              performance on dev data.
    """
    def __init__(self, *, dataset, hp):

        data.Generator.__init__(self, dataset, hp)

        self.include_words = 'words' in hp.features
        self.include_bytes = 'bytes' in hp.features
        self.include_chars = 'chars' in hp.features
        self.include_subwords = 'subwords' in hp.features

        self.char_pad_len = hp.char_pad_len
        self.subword_pad_len = hp.subword_pad_len
        self.drop_single_rate = hp.drop_single_rate

        vocabs = get_vocabs(hp.vocab_file, hp.subword_file)
        self.word_idxr = vocabs['word']
        self.char_idxr = vocabs['chars']
        self.output_idxr = vocabs['label']
        self.sentencepiece_model = vocabs['subword-sp-model']
        self.singleton_list = vocabs['singletons']

        self.embeddings = vocabs['word-embeddings']

        num_labels = len(self.output_idxr)
        hp.set_hparam('output_dim', num_labels)

        num_words = len(self.word_idxr)
        hp.set_hparam('word_voc_size', num_words)

        num_chars = len(self.char_idxr)
        hp.set_hparam('char_voc_size', num_chars)

        if self.sentencepiece_model is not None:
            num_subwords = len(self.sentencepiece_model)
            hp.set_hparam('subword_voc_size', num_subwords)


    def iterate(self):
        """ Iterate over all sentences in a corpus """
        for sentence in self.dataset.get_sentences():
            sentence_id = sentence.id
            words = sentence.words
            tags = sentence.tags
            target = self.output_idxr.index(tags)

            features = {
                Features.INPUT_SEQUENCE_LENGTH.value: [len(words)],
                Features.WINDOW_ID.value: [sentence_id]
            }

            if self.include_words:
                idxd_words = self.word_idxr.index_with_replace(
                    words,
                    self.singleton_list,
                    self.drop_single_rate)

                features[Features.INPUT_WORDS.value] = idxd_words

            if self.include_chars:
                concat_chars, char_lens = self.words_to_concat_chars(words)
                features[Features.INPUT_WORD_CHARS.value] = concat_chars
                features[Features.INPUT_CHAR_LENGTHS.value] = char_lens

            if self.include_bytes:
                concat_bytes, byte_lens = self.words_to_concat_bytes(words)
                features[Features.INPUT_WORD_BYTES.value] = concat_bytes
                features[Features.INPUT_BYTE_LENGTHS.value] = byte_lens

            if self.include_subwords:
                subwords, subword_lens = self.words_to_concat_subwords(words)
                features[Features.INPUT_WORD_SUBWORDS.value] = subwords
                features[Features.INPUT_SUBWORD_LENGTHS.value] = subword_lens

            yield (features, target)

    def words_to_concat_subwords(self, words):
        """ Convert words to concatenated subwords """
        concat_subws = []
        subword_lens = []

        pad_idx = self.sentencepiece_model.pad_id()

        for word in words:
            subwords = self.string_to_subword_list(word)
            if len(subwords) < self.subword_pad_len:
                subword_lens.append(len(subwords))
                n_pad = self.subword_pad_len - len(subwords)
                subwords += [pad_idx] * n_pad
            elif len(subwords) > self.subword_pad_len:
                subwords = subwords[:self.subword_pad_len]
                subword_lens.append(len(subwords))
            else:
                subword_lens.append(len(subwords))
            assert len(subwords) == self.subword_pad_len
            concat_subws += subwords
        return concat_subws, subword_lens

    def string_to_subword_list(self, string):
        """ Convert string to subword list """

        return self.sentencepiece_model.EncodeAsIds(string)

    def words_to_concat_bytes(self, words):
        """ Convert a list of words into a list of list of bytes, and a list of
        those byte lengths.
        """
        concat_bytes = []
        byte_lens = []

        for word in words:
            word_bytes = self.string_to_byte_list(word)
            if len(word_bytes) < self.char_pad_len:
                byte_lens.append(len(word_bytes))
                n_pad = self.char_pad_len - len(word_bytes)
                word_bytes += [0] * n_pad
            elif len(word_bytes) > self.char_pad_len:
                word_bytes = word_bytes[:self.char_pad_len]
                byte_lens.append(len(word_bytes))
            else:
                byte_lens.append(len(word_bytes))
            assert len(word_bytes) == self.char_pad_len
            concat_bytes += word_bytes
        return concat_bytes, byte_lens

    def words_to_concat_chars(self, words):
        """ Converts words to concat chars """

        concat_bytes = []
        byte_lens = []

        pad_key = self.char_idxr.index_of('~@@PAD@@~')
        for word in words:
            word_bytes = self.string_to_char_list(word)
            if len(word_bytes) < self.char_pad_len:
                byte_lens.append(len(word_bytes))
                n_pad = self.char_pad_len - len(word_bytes)
                word_bytes += [pad_key] * n_pad
            elif len(word_bytes) > self.char_pad_len:
                word_bytes = word_bytes[:self.char_pad_len]
                byte_lens.append(len(word_bytes))
            else:
                byte_lens.append(len(word_bytes))
            assert len(word_bytes) == self.char_pad_len
            concat_bytes += word_bytes
        return concat_bytes, byte_lens

    def string_to_char_list(self, string):
        """ Convert string to list of characters """

        return self.char_idxr.index(string)


    @classmethod
    def string_to_byte_list(cls, string):
        """ Convert a string into a list of the bytes representing it.
        Also removed the multlingual prefix, if necessary.
        """
        b = bytes(string, encoding='utf-8')
        return list(b)

    def estimator_params(self):
        return {
            'word_embeddings': self.embeddings
        }

    def datashape(self):
        """ Data shape """

        features = {
            Features.INPUT_SEQUENCE_LENGTH.value: [1],
            Features.WINDOW_ID.value: 1
        }

        if self.include_words:
            features[Features.INPUT_WORDS.value] = [None]

        if self.include_chars:
            features[Features.INPUT_WORD_CHARS.value] = [None]
            features[Features.INPUT_CHAR_LENGTHS.value] = [None]

        if self.include_bytes:
            features[Features.INPUT_WORD_BYTES.value] = [None]
            features[Features.INPUT_BYTE_LENGTHS.value] = [None]

        if self.include_subwords:
            features[Features.INPUT_WORD_SUBWORDS.value] = [None]
            features[Features.INPUT_SUBWORD_LENGTHS.value] = [None]

        return (features, [None])

    def datatypes(self):
        """ Return data types """
        features = {
            Features.INPUT_SEQUENCE_LENGTH.value: tf.int64,
            Features.WINDOW_ID.value: tf.int64
        }

        if self.include_words:
            features[Features.INPUT_WORDS.value] = tf.int64

        if self.include_chars:
            features[Features.INPUT_WORD_CHARS.value] = tf.int64
            features[Features.INPUT_CHAR_LENGTHS.value] = tf.int64

        if self.include_bytes:
            features[Features.INPUT_WORD_BYTES.value] = tf.int64
            features[Features.INPUT_BYTE_LENGTHS.value] = tf.int64

        if self.include_subwords:
            features[Features.INPUT_WORD_SUBWORDS.value] = tf.int64
            features[Features.INPUT_SUBWORD_LENGTHS.value] = tf.int64

        return (features, tf.int64)


@Registries.predictors.register("standard_word_crf")
class CrfModelPredictor(data.Predictor):
    """ Classic Model Predictor.

    This predictor is designed to take sentence-level word-tag predictions.
    The prediction here is extremely simple, for each sentence you find it's
    corresponding predictions (via sentence-id), and you decode the labels for
    each word and store the result.
    """

    def __init__(self, dataset, hp):
        data.Predictor.__init__(self, dataset, hp)
        vocabs = get_vocabs(hp.vocab_file, hp.subword_file)
        self.output_idxr = vocabs['label']

    def gather(self, predictions):
        sentences = self.dataset.get_sentences()
        count = 0

        for pred in predictions:
            sentence = sentences[count]
            count += 1
            predicted_tags = [self.output_idxr.key_of(pred_idx) for
                              pred_idx in pred[Predictions.TAGS.value]]
            predicted_tags = predicted_tags[:pred[Predictions.LENGTH.value]]

            assert len(predicted_tags) == len(sentence.tag_list)

            word_predictions = []

            for i, word in enumerate(sentence.word_list):
                tag = sentence.tag_list[i]
                pred = predicted_tags[i]
                word_predictions.append((word, tag, pred))

            self.sentence_predictions.append(word_predictions)
